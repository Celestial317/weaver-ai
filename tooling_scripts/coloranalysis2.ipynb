{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"13-Vhq8SuPKiRzkfVb4ZQw5K17tQyu3ka","authorship_tag":"ABX9TyMkQ4SENAjz6TYVzA2IkVBN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","\n","csv_path = '/content/annotations.csv'  # Update this path if needed\n","df = pd.read_csv(csv_path)\n","\n","# Filter only training partition\n","train_df = df[df['partition'] == 'train']\n"],"metadata":{"id":"mp4qf28Sk-Iw","executionInfo":{"status":"ok","timestamp":1752500007679,"user_tz":-180,"elapsed":19,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"sc_yywW4lJv7"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, root_dir, transform=None):\n","        self.data = dataframe.reset_index(drop=True)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.class2idx = {k: i for i, k in enumerate(self.data['class'].unique())}\n","        self.subclass2idx = {k: i for i, k in enumerate(self.data['sub_class'].unique())}\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        path = self.data.iloc[idx]['path_rgb_original']\n","        if path.startswith(\"RGB/\"):\n","            path = path[4:]\n","        img_path = os.path.join(self.root_dir, path)\n","        image = Image.open(img_path).convert('RGB')\n","        label_class = self.class2idx[self.data.iloc[idx]['class']]\n","        label_subclass = self.subclass2idx[self.data.iloc[idx]['sub_class']]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label_class, label_subclass\n"],"metadata":{"id":"iv5ctu8hlJ5x","executionInfo":{"status":"ok","timestamp":1752500009024,"user_tz":-180,"elapsed":14,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","root_dir = '/content/drive/MyDrive/dataset'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","# Use only training rows\n","dataset = CustomDataset(dataframe=train_df, root_dir=root_dir, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n"],"metadata":{"id":"sNcg2pr-lVtV","executionInfo":{"status":"ok","timestamp":1752500010832,"user_tz":-180,"elapsed":6,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torchvision.models as models\n","import torch.nn as nn\n","\n","class MultiOutputResNet(nn.Module):\n","    def __init__(self, num_classes, num_subclasses):\n","        super().__init__()\n","        self.base = models.resnet18(weights='IMAGENET1K_V1')\n","        self.base.fc = nn.Identity()\n","        self.fc_class = nn.Linear(512, num_classes)\n","        self.fc_subclass = nn.Linear(512, num_subclasses)\n","\n","    def forward(self, x):\n","        features = self.base(x)\n","        out_class = self.fc_class(features)\n","        out_subclass = self.fc_subclass(features)\n","        return out_class, out_subclass\n","\n","num_classes = len(dataset.class2idx)\n","num_subclasses = len(dataset.subclass2idx)\n","model = MultiOutputResNet(num_classes, num_subclasses)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7i3HgUpkBPCh","executionInfo":{"status":"ok","timestamp":1752500014192,"user_tz":-180,"elapsed":780,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}},"outputId":"eff5a52e-2637-4dbf-b82a-03ce22412bdc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 121MB/s]\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import os\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 10  # adjust as needed\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    num_batches = 0\n","    for images, labels_class, labels_subclass in dataloader:\n","        images = images.to(device)\n","        labels_class = labels_class.to(device)\n","        labels_subclass = labels_subclass.to(device)\n","\n","        optimizer.zero_grad()\n","        out_class, out_subclass = model(images)\n","        loss_class = F.cross_entropy(out_class, labels_class)\n","        loss_subclass = F.cross_entropy(out_subclass, labels_subclass)\n","        loss = loss_class + loss_subclass\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        num_batches += 1\n","\n","    avg_loss = running_loss / num_batches\n","    print(f\"Epoch {epoch+1}/{num_epochs} completed - Loss: {avg_loss:.4f}\")\n","\n","# Save the model weights\n","torch.save(model.state_dict(), '/content/drive/MyDrive/dataset/model_weights2.pth')\n","print(\"Model weights saved!\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLUoJLONBVDk","executionInfo":{"status":"ok","timestamp":1752501465058,"user_tz":-180,"elapsed":1434761,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}},"outputId":"510d09bc-bad6-4317-b120-26db68120c7b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 completed - Loss: 2.4642\n","Epoch 2/10 completed - Loss: 1.5350\n","Epoch 3/10 completed - Loss: 0.7390\n","Epoch 4/10 completed - Loss: 0.2636\n","Epoch 5/10 completed - Loss: 0.1220\n","Epoch 6/10 completed - Loss: 0.0669\n","Epoch 7/10 completed - Loss: 0.0513\n","Epoch 8/10 completed - Loss: 0.0739\n","Epoch 9/10 completed - Loss: 0.1037\n","Epoch 10/10 completed - Loss: 0.0871\n","Model weights saved!\n"]}]},{"cell_type":"code","source":["import torch\n","from torchvision import transforms\n","from PIL import Image\n","import pandas as pd\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 1. Get image path\n","#csv = pd.read_csv('/content/annotations.csv')\n","#row = csv[(csv['partition'] == 'test')].iloc[0]\n","#img_path = '/content/drive/MyDrive/dataset/' + row['path_rgb_original'][6:]\n","img_path = '/content/sofia.jpg'\n","print(\"Testing image path:\", img_path)\n","\n","# 2. Define transform\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","# 3. Load and preprocess image\n","image = Image.open(img_path).convert('RGB')\n","image_tensor = transform(image).unsqueeze(0).to(device)\n","\n","# 4. Load model\n","model = MultiOutputResNet(num_classes, num_subclasses)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/dataset/model_weights.pth', map_location=device))\n","model.to(device)\n","model.eval()\n","\n","# 5. Inference\n","with torch.no_grad():\n","    out_class, out_subclass = model(image_tensor)\n","    pred_class = torch.argmax(out_class, dim=1).item()\n","    pred_subclass = torch.argmax(out_subclass, dim=1).item()\n","\n","# 6. Map to names\n","idx2class = {v: k for k, v in dataset.class2idx.items()}\n","idx2subclass = {v: k for k, v in dataset.subclass2idx.items()}\n","\n","print(\"Predicted class:\", idx2class[pred_class])\n","print(\"Predicted subclass:\", idx2subclass[pred_subclass])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDlf8XKzE6k_","executionInfo":{"status":"ok","timestamp":1752501929723,"user_tz":-180,"elapsed":515,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}},"outputId":"2ed590c2-6fc8-488d-d3d4-48799950bc5b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing image path: /content/sofia.jpg\n","Predicted class: autunno\n","Predicted subclass: soft\n"]}]},{"cell_type":"code","source":["# Make sure your model class and mappings are defined as during training\n","model = MultiOutputResNet(num_classes, num_subclasses)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/dataset/model_weights.pth', map_location=device))\n","model.to(device)\n","model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sg2BdApqF0x6","executionInfo":{"status":"ok","timestamp":1751207336079,"user_tz":-180,"elapsed":567,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}},"outputId":"75697f3b-70af-464a-a041-d1528b1bd269"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiOutputResNet(\n","  (base): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Identity()\n","  )\n","  (fc_class): Linear(in_features=512, out_features=4, bias=True)\n","  (fc_subclass): Linear(in_features=512, out_features=6, bias=True)\n",")"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["with torch.no_grad():\n","    out_class, out_subclass = model(image_tensor)\n","    pred_class_idx = torch.argmax(out_class, dim=1).item()\n","    pred_subclass_idx = torch.argmax(out_subclass, dim=1).item()\n"],"metadata":{"id":"enNff3oqF2mZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YjxSnLgF41q","executionInfo":{"status":"ok","timestamp":1751207380819,"user_tz":-180,"elapsed":30,"user":{"displayName":"hafsah alimuddin","userId":"05054694353794176638"}},"outputId":"ea2f196d-2d0a-4d74-8108-7ca119b8b4e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: autunno\n","Predicted subclass: deep\n"]}]}]}